{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#  for roberta\n",
    "def hist(data, alpha=0.8):\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            plt.hist(value, bins=30, alpha=alpha, label=key)\n",
    "    else:\n",
    "        # 绘制直方图\n",
    "        plt.hist(data, bins=30, color='skyblue', alpha=alpha)\n",
    "\n",
    "    # 设置图表属性\n",
    "    plt.title('weights')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "\n",
    "    # 显示图表\n",
    "    plt.show()\n",
    "    \n",
    "def weights_plot(data, alpha=0.8):\n",
    "    keys = list(data.keys())\n",
    "    title = f\"Weight Analysis of {keys[0].split('_')[0]}\"\n",
    "    shape = list(list(data.values())[0].shape)\n",
    "    length = len(data)\n",
    "    x = int(np.sqrt(length))\n",
    "    y = int(np.ceil(length / x))\n",
    "    fig, axes = plt.subplots(x, y, sharex=True, sharey=True, figsize=(16, 16))\n",
    "    fig.suptitle(title)\n",
    "    \n",
    "    vmin = 0\n",
    "    vmid = 0\n",
    "    for value in list(data.values()):\n",
    "        values_ = torch.sort(value.view(-1))\n",
    "        index_ = int(values_[0].shape[0] * 0.7)\n",
    "        vmid_  = values_[0][index_]\n",
    "        if vmid_ > vmid:\n",
    "            vmid = vmid_\n",
    "    images = []\n",
    "    index = 0\n",
    "    for key, value in data.items():\n",
    "        \n",
    "        if x == 1:\n",
    "            im = axes[index].imshow(value, vmin=vmin, vmax=vmid*2, aspect='auto')\n",
    "            images.append(im)\n",
    "            axes[index].set_title(f\"{key.split('_')[2]}\")\n",
    "        else:\n",
    "            x_ = index // y\n",
    "            y_ = index % y\n",
    "            im = axes[x_][y_].imshow(value, vmin=vmin, vmax=vmid*2, aspect='auto')\n",
    "            images.append(im)\n",
    "            axes[x_][y_].set_title(f\"{key.split('_')[2]}\")\n",
    "            \n",
    "        \n",
    "        index += 1\n",
    "    fig.colorbar(images[0], ax=axes, orientation='horizontal', fraction=.1)\n",
    "    plt.savefig(f\"{title}.jpg\")\n",
    "    \n",
    "def weight_key_mapping(weight_data, model_name):\n",
    "    key_mapping = {\n",
    "        \"roberta-base\": \"layer\",\n",
    "        \"llama2\": \"layers\"\n",
    "    }\n",
    "    module_index_mapping = {\n",
    "        \"roberta-base\": 3,\n",
    "        \"llama2\": 2\n",
    "    }\n",
    "    keys = list(weight_data.keys())\n",
    "\n",
    "    layer_mapping = {}\n",
    "    for item in keys:\n",
    "        # print(item)\n",
    "        if key_mapping[model_name] in item:\n",
    "            key_suffixs = item.split(f\"{key_mapping[model_name]}.\")[-1].split(\".\")\n",
    "            index = int(key_suffixs[0])\n",
    "            module_name = key_suffixs[module_index_mapping[model_name]]\n",
    "            # print(item)\n",
    "            key = \"_\".join([\"layer\", str(index), module_name])\n",
    "            lora_key = \"a\" if \"lora_A\" in item else \"b\"\n",
    "            if key not in layer_mapping:\n",
    "                layer_mapping[key] = {\n",
    "                    lora_key: item\n",
    "                }\n",
    "            else:\n",
    "                layer_mapping[key][lora_key] = item\n",
    "    return layer_mapping\n",
    "\n",
    "def weight_anaylsis(weight, model_name=\"roberta-base\", interval=5):\n",
    "    key_mapping = {\n",
    "        \"roberta-base\": {\n",
    "            \"q\": \"query\",\n",
    "            \"k\": \"key\",\n",
    "            'v': \"value\"\n",
    "        },\n",
    "        \"llama2\": {\n",
    "            \"q\": \"q_proj\",\n",
    "            \"k\": \"k_proj\",\n",
    "            'v': \"v_proj\"\n",
    "        }\n",
    "    }\n",
    "    layer_mapping = weight_key_mapping(weight, model_name)\n",
    "    # for key, value in layer_mapping.items():\n",
    "    #         # print(layer_mapping[key])\n",
    "    #         weight_A = weight[value['a']]\n",
    "    #         weight_B = weight[value['b']]\n",
    "    #         m = torch.abs(weight_B @ weight_A)\n",
    "    #         print(key, torch.mean(m), torch.max(m), torch.min(m), torch.std(m), torch.median(m))\n",
    "    weights_query_dict = {}\n",
    "    weights_value_dict = {}\n",
    "    for key, value in layer_mapping.items():\n",
    "        # print(layer_mapping[key])\n",
    "        \n",
    "        # print(key, torch.mean(m), torch.max(m), torch.min(m), torch.std(m))\n",
    "        # print(key)\n",
    "        index = int(key.split(\"_\")[1])\n",
    "        if index % interval == 0:\n",
    "            if key_mapping[model_name][\"q\"] in key:\n",
    "                # print(key_mapping[model_name][\"q\"], key, value['a'])\n",
    "                weight_A = weight[value['a']].float()\n",
    "                weight_B = weight[value['b']].float()\n",
    "                m = torch.abs(weight_B @ weight_A)\n",
    "                # print(value['a'], m.numel())\n",
    "                weights_query_dict[f\"query_{key}\"] = m\n",
    "            if key_mapping[model_name][\"v\"] in key:\n",
    "                # print(key)\n",
    "                weight_A = weight[value['a']].float()\n",
    "                weight_B = weight[value['b']].float()\n",
    "                m = torch.abs(weight_B @ weight_A)\n",
    "                # print(value['a'], m.numel())\n",
    "                weights_value_dict[f\"value_{key}\"] = m\n",
    "\n",
    "    weights_plot(weights_query_dict, alpha=0.2)\n",
    "    weights_plot(weights_value_dict, alpha=0.2)\n",
    "\n",
    "roberta_weight_lora_path_mapping = {\n",
    "    \"sst2\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/sst2/LORA/step_1320/adapter_model.bin\",\n",
    "    \"mnli\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/mnli/LORA/step_3068/adapter_model.bin\",\n",
    "    \"cola\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/cola/LORA/step_680/adapter_model.bin\",\n",
    "    \"mrpc\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/mrpc/LORA/step_300/adapter_model.bin\",\n",
    "    \"qnli\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/qnli/LORA/step_8200/adapter_model.bin\",\n",
    "    \"qqp\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/qqp/LORA/step_28440/adapter_model.bin\",\n",
    "    \"rte\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/rte/LORA/step_200/adapter_model.bin\",\n",
    "    \"stsb\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/stsb/LORA/step_460/adapter_model.bin\",\n",
    "    \"wnli\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/wnli/LORA/step_60/adapter_model.bin\",\n",
    "}\n",
    "\n",
    "dt_name = [\"sst2\", \"mnli\", \"cola\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"stsb\", \"wnli\"]\n",
    "# dt_name = [\"sst2\"]\n",
    "for dt in dt_name:\n",
    "    print(f\"{dt}\")\n",
    "    weight = torch.load(roberta_weight_lora_path_mapping[dt], map_location='cpu')\n",
    "    weight_anaylsis(weight, interval=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "roberta_weight_lora_path_mapping = {\n",
    "    \"sst2\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/sst2/LORA/step_1320/adapter_model.bin\",\n",
    "    \"mnli\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/mnli/LORA/step_3068/adapter_model.bin\",\n",
    "    \"cola\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/cola/LORA/step_680/adapter_model.bin\",\n",
    "    \"mrpc\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/mrpc/LORA/step_300/adapter_model.bin\",\n",
    "    \"qnli\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/qnli/LORA/step_8200/adapter_model.bin\",\n",
    "    \"qqp\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/qqp/LORA/step_28440/adapter_model.bin\",\n",
    "    \"rte\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/rte/LORA/step_200/adapter_model.bin\",\n",
    "    \"stsb\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/stsb/LORA/step_460/adapter_model.bin\",\n",
    "    \"wnli\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/roberta/wnli/LORA/step_60/adapter_model.bin\",\n",
    "}\n",
    "\n",
    "# dt_name = [\"sst2\", \"mnli\", \"cola\", \"mrpc\", \"qnli\", \"qqp\", \"rte\", \"stsb\", \"wnli\"]\n",
    "dt_name = [\"sst2\"]\n",
    "for dt in dt_name:\n",
    "    print(f\"{dt}\")\n",
    "    weight = torch.load(roberta_weight_lora_path_mapping[dt], map_location='cpu')\n",
    "    weight_anaylsis(weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llama2_weight_lora_path_mapping = {\n",
    "    \"sst2\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/llama/sst2/LORA/step_780/adapter_model.bin\",\n",
    "    \"mnli\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/llama/mnli/LORA/step_780/adapter_model.bin\",\n",
    "    \"cola\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/llama/cola/LORA/step_670/adapter_model.bin\",\n",
    "    \"mrpc\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/llama/mrpc/LORA/step_285/adapter_model.bin\",\n",
    "    \"qnli\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/llama/qnli/LORA/step_780/adapter_model.bin\",\n",
    "    \"qqp\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/llama/qqp/LORA/step_780/adapter_model.bin\",\n",
    "    \"rte\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/llama/rte/LORA/step_195/adapter_model.bin\",\n",
    "    \"stsb\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/llama/stsb/LORA/step_450/adapter_model.bin\",\n",
    "    \"wnli\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/llama/wnli/LORA/step_50/adapter_model.bin\",\n",
    "    \"all\": \"/hpc2hdd/home/fwang380/dongjunwei/llm_trainer/model_output/llama/all/LORA/step_4775/adapter_model.bin\"\n",
    "}\n",
    "\n",
    "dt_name = [\"sst2\"]\n",
    "for dt in dt_name:\n",
    "    print(f\"{dt}\")\n",
    "    weight = torch.load(llama2_weight_lora_path_mapping[dt], map_location='cpu')\n",
    "    weight_anaylsis(weight, model_name=\"llama2\", interval=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "peft_djw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
