data-params:
    dataset_class_name: BoolqDataset
    train_file: super_glue-boolq
    eval_file: super_glue-boolq
    # dataset_class_name: CommonDataset
    # train_file: /hpc2hdd/home/fwang380/dongjunwei/llm_trainer/test_data/train_tot_1k.jsonl

model-params:
    peft: False
    pretrain_model_path: "/hpc2hdd/home/fwang380/OpenSource/Models/Llama-2-7b-hf"
    output_model_path: "./model_outputs"
    output_fp32_model: False
    low_cpu_mem_usage: False
    torch_dtype: float16

tokenizer-params:
    pretrain_tokenizer_path: "/hpc2hdd/home/fwang380/OpenSource/Models/Llama-2-7b-hf"
    add_special_tokens: True
    special_tokens:
        bos_token: "<s>"
        eos_token: "</s>"
        sep_token: ""
        pad_token: "<pad>"
        unk_token: ""
    max_len: 1024
    batch_pad_truncate: True # truncate samples in batch level, if true, can set a higger batchsize, the training will significantly accelerate, however, when the data is long, use this may OOM
    ignore_q: True # if ignore the question and prompt in samples

trainer-params:
    wandb_project_name: PEFT_LLM_debug # a project contains a series of runs[`run_name`], which can be shown in webui
    run_name: peft_adalora_lr1e-3_train_external_platform_cosine_v2 # a wandb trail run name
    seed: 42
    data_seed: 42
    dataloader_pin_memory: True
    output_dir: "model_output/debug/external_platform_c2_model_peft_adalora_lr_1e-3_cosine_v2"
    num_train_epochs: 5
    do_eval: True
    eval_fn: "boolq"
    evaluation_strategy: "epoch"
    eval_delay: 0 # from which epoch/step to evaluate
    use_legacy_prediction_loop: True # before transformer v5, set this key to true can ensure that the predicted data and the original dataset are length-aligned
    prediction_loss_only: False # generate no loss
    predict_with_generate: True # predict via generate function
    prediction_file_name: "result.jsonl"
    care_tokens: ['是', '否'] # if care some str probability , after set `generation_config.output_scores` to true, just specify special characters, otherwhise, output each generated tokens' probability
    # per_device_eval_batch_size: 24 # 7B
    per_device_eval_batch_size: 12
    per_device_train_batch_size: 16 # peft:32
    gradient_accumulation_steps: 2
    gradient_checkpointing: True # if True, save more memory in bp stage
    save_strategy: "no" # if not use resume either ft, continue pretrain, try to keep this key 'no'
    # optim: adamw_hf
    learning_rate: 0.00002 # peft:0.0008
    weight_decay: 0.0001
    adam_beta1: 0.9
    adam_beta2: 0.95
    # lr_scheduler_type: cosine
    warmup_ratio: 0.001
    logging_strategy: "steps"
    logging_steps: 1
    log_on_each_node: False # just logging in main node
    bf16: True
    fp16: False
    deepspeed: /hpc2hdd/home/fwang380/dongjunwei/llm_trainer/config/train/ds_config/ds_config_zero2_no_cpu_offload_op_scheduler.json
    # adalora just suport zero1
    remove_unused_columns: False
    combine_causal_loss_factor: 0.1 # combie the ft loss + continue pretrain loss for peft
    generation_config:
        max_length: 512
        max_new_tokens: 5
        do_sample: False # if false, just one, greedy search
        num_beams: 1
        temperature: 0.7 # if do_sample = True, this key works
        top_k: 5
        output_scores: True

peft-params:
    peft_type: lora
    task_type: CAUSAL_LM
    target_modules: ['query_key_value', 'dense_h_to_4h', 'dense_4h_to_h', 'dense']
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    activate_fn: ~
    d_parts: 1