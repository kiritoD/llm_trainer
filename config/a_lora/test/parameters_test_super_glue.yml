data-params:
    dataset_class_name: SuperGlueDatasets
    train_file: 
    # - boolq
    # - multirc
    # - cb
    # - copa
    # - rte
    # - wic
    # - wsc
    - record
    eval_file: 
    # - boolq
    # - multirc
    # - cb
    # - copa
    # - rte
    # - wic
    # - wsc
    - record
    train_size: 100
    eval_size: 50
# data-params:
#     dataset_class_name: BoolqDataset
#     train_file: super_glue-boolq
#     train_size: 1000 # if debug can set train_size
#     eval_file: super_glue-boolq
    # eval_size: 100
# dataset_class_name: ExternalPlatformDataset
#     train_file: /mnt/bn/ecom-ccr-dev/mlx/users/dongjunwei/EasyGuard/examples/fashion_bloom/cle_dataset/content_live_external_platform_train.jsonl
#     eval_file: /mnt/bn/ecom-ccr-dev/mlx/users/dongjunwei/EasyGuard/examples/fashion_bloom/cle_dataset/content_live_external_platform_val.jsonl

model-params:
    peft: True
    pretrain_model_path: "/mnt/bn/ecom-ccr-dev/mlx/users/dongjunwei/hf_models/Llama-2-7b-hf-debug"
    output_model_path: "./model_outputs"
    output_fp32_model: False
    low_cpu_mem_usage: True

tokenizer-params:
    pretrain_tokenizer_path: "/mnt/bn/ecom-ccr-dev/mlx/users/dongjunwei/hf_models/Llama-2-7b-hf-debug"
    add_special_tokens: True
    special_tokens:
        bos_token: "<s>"
        eos_token: "</s>"
        sep_token: ""
        pad_token: "<pad>"
        unk_token: ""
    max_len: 1024
    batch_pad_truncate: True # truncate samples in batch level, if true, can set a higger batchsize, the training will significantly accelerate, however, when the data is long, use this may OOM
    ignore_q: True # if ignore the question and prompt in samples

trainer-params:
    wandb_project_name: PEFT_LLM_debug # a project contains a series of runs[`run_name`], which can be shown in webui
    run_name: lora_r2_sumsam_debug_super_glue_each # a wandb trail run name
    seed: 42
    data_seed: 42
    dataloader_pin_memory: True
    output_dir: "model_output/local_debug/test_super_glue"
    num_train_epochs: 2
    do_eval: True
    evaluation_strategy: "steps"
    eval_steps: 5
    # eval_fn: "cnn_dailymail_metrics"
    eval_fn: "super_glue_metric"
    eval_delay: 0 # from which epoch/step to evaluate
    use_legacy_prediction_loop: True # before transformer v5, set this key to true can ensure that the predicted data and the original dataset are length-aligned
    predict_with_generate: True # predict via generate function
    prediction_loss_only: False # generate no loss
    prediction_file_name: "result.jsonl"
    care_tokens: ~ # if care some tokens' probability , after set `generation_config.output_scores` to true, just specify special characters, otherwhise, output each generated tokens' probability
    per_device_eval_batch_size: 16
    per_device_train_batch_size: 16 # peft:32
    gradient_accumulation_steps: 2
    gradient_checkpointing: True # if True, save more memory in bp stage
    save_strategy: "no" # if not use resume either ft, continue pretrain, try to keep this key 'no'
    optim: adamw_hf # torch >= 2.0
    learning_rate: 0.001 # peft:0.0008
    weight_decay: 0.0001
    adam_beta1: 0.9
    adam_beta2: 0.95
    lr_scheduler_type: cosine
    warmup_ratio: 0.001
    logging_strategy: "steps"
    logging_steps: 1
    log_on_each_node: False # just logging in main node
    bf16: True
    fp16: False
    deepspeed: /mnt/bn/ecom-ccr-dev/mlx/users/dongjunwei/EasyGuard/examples/peft_llm/config/train/ds_config/ds_config_zero1_no_cpu_offload_op_scheduler.json
    # adalora just suport zero1
    remove_unused_columns: False
    only_causal_loss: False
    combine_causal_loss_factor: 0.1 # combie the ft loss + continue pretrain loss for peft
    # generation_max_length: 512 # you should set this value to a suitable value beacause in the ealy epoch, model may generate max-length string beacuse of its incomprehension
    generation_config:
        max_length: 1024
        max_new_tokens: 10
        do_sample: False # if false, just one, greedy search
        num_beams: 1
        temperature: 0.7 # if do_sample = True, this key works
        top_k: 5
        output_scores: True
peft-params:
    peft_type: lora
    task_type: CAUSAL_LM
    target_modules: ['q_proj', 'down_proj', 'gate_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'lm_head']
    r: 12
    lora_alpha: 24
    lora_dropout: 0.05
    activate_fn: ~
    d_parts: 2
# peft-params:
#     peft_type: naslora
#     task_type: CAUSAL_LM
#     target_modules: ['q_proj', 'down_proj', 'gate_proj', 'k_proj', 'v_proj', 'o_proj', 'up_proj', 'lm_head']
#     nas_search_space: /mnt/bn/ecom-ccr-dev/mlx/users/dongjunwei/EasyGuard/examples/peft_llm/config/search_space/common.yml
#     reset_mode: 2
#     search_strategy: "step"
#     search_step: 30
#     reg_loss: True
#     naslora_weights_lr: 0.01
#     # search_policy: "adaptive"
#     # policy_params:
#     #     patience: 3
#     #     threshold_1: 0.1
#     #     threshold_2: 0.02
#     search_policy: "independent"
#     policy_params:
#         pretrain_step: 2
#         step_1: 1
#         step_2: 2
#     naslora_dropout: 0.05
#     top_n: 1